# jemdoc: menu{MENU}{projects.html}
= Projects

== Analysis and Control of Hybrid Dynamical Systems

A hybrid system is a dynamical system that exhibits both continuous and discrete dynamic behavior, a system that can both flow (described by a differential equation) and jump (described by a state machine or automaton). The hybrid system framework offers an elegant way to model complex many technological systems, in which logic decision making and embedded control actions are combined with continuous physical processes such as a  variety of  Cyber-Physical Systems (CPS), e.g., collision avoidance protocols in air traffic control,  obstacle avoidance algorithms,  software-controlled medical devices, etc.  

\n\n

I am personally interested in the development of new theoretical results for the analysis and control of hybrid systems. My recent work proposed novel stability criteria for hybrid systems with time-dependent switching in the framework of multiple Lyapunov functions, including non-conservative stability criteria, formulated in terms of LP, SDP, and SOS conditions.
 The results are further developed to a variety of hybrid systems such as stochastic switching systems, positive hybrid systems, uncertain hybrid systems, etc, as well as control synthesis of hybrid systems, e.g., event-triggered control.  

\n
~~~
{}{img_left}{image/hybrid_system.png}{alt text}{650px}{380px}{}
~~~
\n

Figures are from [https://drive.google.com/open?id=1phmuHqLQq21zV9-3qHavgiRCEn1ggNim Automatica (2014)], [https://drive.google.com/open?id=1qPGSVbrI_fLJw8IuGcA7pqFiXnArn9aK Automatica (2018)].

== Verification of Neural Network Systems

Artificial neural networks are used in systems that introduce machine learning components to resolve complex neural networks problems. This can be attributed to the impressive ability to approximate complex functions as shown by the Universal Approximation Theorem. However, the data-driven nature and lack of efficient methods for the analysis of neural networks leads to, in most cases, the treatment of neural networks as black boxes with no assurance in safety. Verifying neural networks is a difficult problem, and it has been demonstrated that validating even simple properties about their behavior is NP-complete. I am personally interested in developing verification algorithms in the framework of reachability of neural networks. The developed set-valued reachability framework aims to compute the output set of a neural network, then system properties such as safety property can be verified by checking the relationship, e.g., intersection, between output set and specification set.
\n
~~~
{}{img_left}{image/nn_verification_1.png}{alt text}{480px}{250px}{}
~~~
\n

The figure is from [https://drive.google.com/file/d/1P_nb7ZfZ6-mVqzlg5_kDL1qzNUWeECh4/view IEEE Design and Test (2020)].
\n\n

I am developing efficient and scalable reachability-based verification frameworks of neural networks. Simulation-guided methods incorporated with interval arithmetic are proposed for neural networks with general activation functions. Polyhedra computation framework is developed for neural networks with ReLU activation function.

\n
~~~
{}{img_left}{image/nn_verification_2.png}{alt text}{470px}{470px}{}
~~~
\n

Figures are from [https://arxiv.org/pdf/2004.12273.pdf IEEE TNNLS (2020)] and [https://arxiv.org/pdf/2004.05519.pdf CAV (2020)].


== Trustworthy Data-Driven Modeling and Control of Cyber-Physical Systems

Modern control systems, e.g., medical robotic systems, autonomous vehicles, and a variety of Cyber-Physical Systems (CPS), have been increasingly benefiting from the recent rapid development of Machine Learning (ML) and Artificial Intelligence (AI) techniques. In the foreseeable future, there will be intensive interactions between ML and dynamical control system domains in various stages of modeling, sensing, and control. Over the next decade, the biggest generator of data is expected to be devices that sense and control the physical world. Effective and trustworthy data-driven methods are required to meet and handle the explosion of data emerging from the physical world. Moreover, with the progress of adversarial machine learning, traditional methods dealing with uncertainties and disturbances such as robust control methods are no longer valid as long as the perturbations and attacks might be purposefully crafted by adversarial opponents rather than random noise. Thus, new assured ML methods and processes are required for the data-driven modeling and control processes of CPS to assure formal guarantees of desired properties.
\n
\n

I am personally interested in developing post-modeling and design verification methods as well as correct-by-design methods to achieve the trustworthiness of data-driven modeling and control of CPS. 

\n
~~~
{}{img_left}{image/data_driven.png}{alt text}{630px}{520px}{}
~~~
\n

Figures are from [https://arxiv.org/pdf/1802.03557.pdf Safe, Autonomous and Intelligent Vehicles (2019)] and [https://arxiv.org/pdf/2004.12273.pdf IEEE TNNLS (2020)].


== Run-Time Monitoring of Learning-Enabled Cyber-Physical Systems

To assure the safety property of neural networks, there are a  few safety verification methods developed recently.  These approaches are mostly designed in the framework of offline computation and usually represent high computational complexities and require huge computation resources to conduct safety verification. The developed methods are expected to be able to conduct scalable design-time verification for hybrid systems with learning-enabled components. One way to resolve the real-time challenge of run-time monitoring is to develop more computational efficient verification methods that can be executed sufficiently fast to satisfy the run-time requirement such as the specification-guide method and polyhedral set methods.  However,  these offline methods are essential with an open-loop computation structure and there always exist computational limitations for these offline algorithms implemented online.  On the other hand, inspired by observer design techniques in classical control theory, another way is to design a closed-loop structure of run-time monitoring using the instantaneous measurement of the system. 

\n
~~~
{}{img_left}{image/runtime.gif}{alt text}{750px}{350px}{}
~~~
\n

The figure is for runtime safety monitoring of an adaptive cruise control system with a neural network feedback controller. Figures are from [https://arxiv.org/pdf/2101.08297.pdf IEEE Transactions on Cybernetics (2021)].

== Assured Neural Network Compression

Model reduction is a plausible way to enhance scalability for model-based system design. The experimental results suggest systems with thousands of state variables can be reduced to systems with tens of state variables such that the order-reduction over-approximation error is small enough to prove or disprove formal properties of interest using reachability analysis tools. Given a large-scale neural network which is difficult for us to perform analysis and verification, the assured neural network compression provides a new neural network with a much smaller size as well as a guaranteed error distance to the original neural network, so that we can perform analysis and verification of properties on the new reduced-size neural network and map the results back to the original neural network with the desired degree of assurance.
\n\n

I am personally interested in developing a bisimulation framework to characterize the bisimilarity between two neural networks and use the reachability-based framework to compute the bisimulation errors.
\n\n
~~~
{}{img_left}{image/nn_reduction.png}{alt text}{700px}{340px}{}
~~~
\n 